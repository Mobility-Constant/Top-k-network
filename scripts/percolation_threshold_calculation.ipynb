{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percolation Threshold Calculation and Following Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from shapely import wkt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "# font setting\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial']  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: Find cbgs in the  water\n",
    "This is because there are some cgbs that are in the water in our original dataset (id_dict and flow matrix). This phenomenon is mostly in NewYork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbgs = gpd.read_file(r\".\\arcgis project\\cbgs\\cbgs_of_cities\\NewYork\\New_York_city.shp\")\n",
    "with open(r'.\\data\\Mobility\\id_dict_1.pkl', 'rb') as f:\n",
    "    id_dict = pickle.load(f)\n",
    "    \n",
    "print(cbgs.info())\n",
    "print(len(id_dict.keys()))\n",
    "\n",
    "# Extract the lines that not in cbgs but in id_dict\n",
    "id_dict_keys = list(id_dict.keys())\n",
    "id_dict_values = list(id_dict.values())\n",
    "cbgs_keys = list(cbgs['CBG_Code'].values)\n",
    "cbgs_in_water = []\n",
    "count = 0\n",
    "for i in id_dict_keys:\n",
    "    if id_dict[id_dict_keys[i]] not in cbgs_keys:\n",
    "        cbgs_in_water.append(i)\n",
    "        count += 1\n",
    "#cbgs_in_water"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percolation Threshold Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "def minimum_flows_for_strongly_connectivity(flow_matrix, percentile=2):\n",
    "    \"\"\"The algorithm to find the smallest k such that the subgraph remains strongly connected\n",
    "       after removing the nodes with the smallest degree.\n",
    "       \n",
    "    Args:\n",
    "        flow_matrix (numpy array): Flow matrix of the graph.\n",
    "\n",
    "    Returns:\n",
    "        ks: List of the smallest k such that the subgraph remains strongly connected.\n",
    "    \"\"\"\n",
    "    ks = []\n",
    "    # Function to check if the graph is strongly connected when only top k edges are kept for each node\n",
    "    def check_strongly_connected_k(graph, k):\n",
    "        # Create a new graph which only keeps top k outgoing edges for each node\n",
    "        new_graph = nx.DiGraph()\n",
    "        for node in graph.nodes():\n",
    "            edges = sorted(graph.out_edges(node, data=True), key=lambda x: x[2]['weight'], reverse=True)[:k]\n",
    "            new_graph.add_edges_from(edges)\n",
    "        # Check if the new graph is still strongly connected\n",
    "        return nx.is_strongly_connected(new_graph)\n",
    "    \n",
    "    # Set the diagonal elements to zero\n",
    "    np.fill_diagonal(flow_matrix, 0)\n",
    "\n",
    "    # Create directed graph from flow matrix\n",
    "    G = nx.from_numpy_array(flow_matrix, create_using=nx.DiGraph)\n",
    "\n",
    "    # Convert graph to numpy adjacency matrix\n",
    "    adj_matrix = nx.adjacency_matrix(G)\n",
    "    numpy_array = adj_matrix.toarray()\n",
    "    # Convert the original flows to binary\n",
    "    flows_binary = (numpy_array > 0).astype(int)\n",
    "    # Calculate the degrees based on the new definition\n",
    "    degrees_all = np.sum(np.logical_or(flows_binary, flows_binary.T), axis=1)\n",
    "\n",
    "    # Determine the 2% degree threshold\n",
    "    percentile_threshold = np.percentile(degrees_all, percentile)\n",
    "\n",
    "    # find the nodes whose degree are in the lowest 2%\n",
    "    nodes_to_remove = np.where(degrees_all <= percentile_threshold)[0]\n",
    "    print('Number of nodes to remove: ', len(nodes_to_remove))\n",
    "\n",
    "    # Remove the nodes from the matrix\n",
    "    reduced_matrix = np.delete(numpy_array, nodes_to_remove, axis=0)\n",
    "    reduced_matrix = np.delete(reduced_matrix, nodes_to_remove, axis=1)\n",
    "\n",
    "    # Create directed graph from the reduced matrix\n",
    "    reduced_G = nx.from_numpy_array(reduced_matrix, create_using=nx.DiGraph)\n",
    "    \n",
    "    # Ensure the graph is strongly connected\n",
    "    scc = list(nx.strongly_connected_components(reduced_G))\n",
    "    strongly_connected_G = reduced_G.subgraph(max(scc, key=len))\n",
    "    num_units = len(strongly_connected_G.nodes())\n",
    "\n",
    "    # Binary search variables\n",
    "    low = 1\n",
    "    high = num_units\n",
    "\n",
    "    while low < high:\n",
    "        mid = low + (high - low) // 2\n",
    "\n",
    "        # Check if the graph is strongly connected for the current mid\n",
    "        if check_strongly_connected_k(strongly_connected_G, mid):\n",
    "            high = mid\n",
    "        else:\n",
    "            low = mid + 1\n",
    "\n",
    "    # Final check if the graph is strongly connected for the low/high point\n",
    "    if check_strongly_connected_k(strongly_connected_G, low):\n",
    "        print(low)\n",
    "        ks.append(low)\n",
    "    else:\n",
    "        print(-1)\n",
    "        ks.append(-1)\n",
    "\n",
    "    return ks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the threshold for each city\n",
    "cities = ['New York, NY', 'Los Angeles, CA', 'Chicago, IL','Phoenix, AZ', 'Philadelphia, PA', 'San Diego, CA', 'Dallas, TX', 'San Jose, CA']\n",
    "city_list = list(range(1,9))\n",
    "delete_percent = 2\n",
    "df_ks = pd.DataFrame(columns = ['year','month']+city_list)\n",
    "\n",
    "for year in range(2018,2022):\n",
    "    for month in range(1,13):\n",
    "        df_ks.loc[len(df_ks),['year', 'month']] = [year, month]\n",
    "        index = len(df_ks)-1\n",
    "        for city in city_list:\n",
    "            if os.path.exists(r'.\\data\\Mobility\\cbg_visit_{year_}-{month_}_{city_}.npy'.format(year_ = year, month_ = str(month).zfill(2), city_ = city)):\n",
    "                flow_matrix = np.load(r'.\\data\\Mobility\\cbg_visit_{year_}-{month_}_{city_}.npy'.format(year_ = year, month_ = str(month).zfill(2), city_ = city))\n",
    "                print(city, cities[city-1])\n",
    "                # Remove the cbgs in the water in New York\n",
    "                if city == 1:\n",
    "                    flow_matrix = np.delete(flow_matrix, cbgs_in_water, axis=0)\n",
    "                    flow_matrix = np.delete(flow_matrix, cbgs_in_water, axis=1)\n",
    "                ks = minimum_flows_for_strongly_connectivity(flow_matrix, delete_percent)\n",
    "                df_ks.loc[index, city] = ks[0]\n",
    "                print(ks)\n",
    "\n",
    "df_ks.columns =['year','month'] + cities\n",
    "df_ks.to_csv('k_select_US_{}percent.csv'.format(delete_percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of network metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of the clustering coefficient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import geopy.distance\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_k_subgraph(strongly_connected_G, k, node_to_cbg, cbg_to_centroid):\n",
    "    new_graph = nx.DiGraph()\n",
    "    edge_count = 0\n",
    "    # 添加边并计算距离\n",
    "    for node in strongly_connected_G.nodes():\n",
    "        edges = sorted(strongly_connected_G.out_edges(node, data=True), key=lambda x: x[2]['weight'], reverse=True)[:k]\n",
    "        for edge in edges:\n",
    "            u, v, data = edge\n",
    "            u_cbg = node_to_cbg.get(u)\n",
    "            v_cbg = node_to_cbg.get(v)\n",
    "            new_graph.add_edge(u, v, weight=data['weight'])\n",
    "    return new_graph\n",
    "\n",
    "\n",
    "cities = ['New York, NY', 'Los Angeles, CA', 'Chicago, IL', 'Phoenix, AZ', 'Philadelphia, PA', 'San Diego, CA', 'Dallas, TX', 'San Jose, CA']\n",
    "results_df = pd.DataFrame(columns=['year', 'month', 'city', 'k', 'avg_distance','cluster_coeff'])\n",
    "\n",
    "all_ks = [5, 10, 15, 20, 30, 50, 70, 90, 110, 130, 150, 200, 250, 300, 400, 500,  600, 900, 950, 1200, 1500, 2000, 2500]\n",
    "# Load the files\n",
    "percentile = 2\n",
    "for city in range(1,9):\n",
    "    # Load the files\n",
    "    # delete results_df \n",
    "    del results_df\n",
    "    results_df = pd.DataFrame(columns=['year', 'month', 'city', 'k','cluster_coeff'])\n",
    "    with open(r\"./data/Mobility/id_dict_{}.pkl\".format(city), 'rb') as f:\n",
    "        node_to_cbg = pickle.load(f)\n",
    "    city_name = cities[city-1].split(',')[0].replace(' ', '_')\n",
    "    gdf = gpd.read_file(r'./arcgis project/cbgs/cbgs_of_cities/{}_city.shp'.format(city_name))\n",
    "    # save precomputed_distances\n",
    "    for year in range(2018, 2022):\n",
    "        for month in range(1,13):\n",
    "            start = time.time()\n",
    "            mobility_matrix = np.load(r'./data/Mobility/cbg_visit_{y}-{m}_{c}.npy'.format(y=year, m=str(month).zfill(2), c=city))\n",
    "            # ks is the list that maximum element smaller than  gdf.shape[0] from all ks\n",
    "            ks = [x for x in all_ks if x < gdf.shape[0]-100]\n",
    "\n",
    "            # Remove the cbgs in the water in New York\n",
    "            if city == 1:\n",
    "                reduced_nodes = not_in_cbgs\n",
    "                # Remove the nodes from the matrix\n",
    "                flow_matrix = np.delete(mobility_matrix, reduced_nodes, axis=0)\n",
    "                flow_matrix = np.delete(flow_matrix, reduced_nodes, axis=1)\n",
    "            else:\n",
    "                flow_matrix = mobility_matrix\n",
    "\n",
    "            np.fill_diagonal(flow_matrix, 0)\n",
    "            numpy_array = flow_matrix\n",
    "            flows_binary = (numpy_array > 0).astype(int)\n",
    "            degrees_all = np.sum(np.logical_or(flows_binary, flows_binary.T), axis=1)\n",
    "\n",
    "            percentile_threshold = np.percentile(degrees_all, percentile)\n",
    "            nodes_to_remove = np.where(degrees_all <= percentile_threshold)[0]\n",
    "\n",
    "            original_indices = np.arange(numpy_array.shape[0])\n",
    "            preserved_indices = np.delete(original_indices, nodes_to_remove)\n",
    "\n",
    "            reduced_matrix = np.delete(numpy_array, nodes_to_remove, axis=0)\n",
    "            reduced_matrix = np.delete(reduced_matrix, nodes_to_remove, axis=1)\n",
    "            reduced_G = nx.from_numpy_array(reduced_matrix, create_using=nx.DiGraph)\n",
    "\n",
    "            scc = list(nx.strongly_connected_components(reduced_G))\n",
    "            max_scc = max(scc, key=len)\n",
    "            preserved_indices = [index for index in preserved_indices if index in max_scc]\n",
    "            strongly_connected_G = reduced_G.subgraph(max(scc, key=len))\n",
    "            # 更新 node_to_cbg 映射\n",
    "            updated_node_to_cbg = {new_index: node_to_cbg[original_index] for new_index, original_index in enumerate(preserved_indices)}\n",
    "\n",
    "            # 初始化用于存储结果的数组\n",
    "            avg_distances = []\n",
    "            pre = time.time()\n",
    "            print(\"pre time: \", pre-start)\n",
    "            for k in tqdm(ks):    \n",
    "                #mid1 = time.time()\n",
    "                graph = get_k_subgraph(strongly_connected_G, k, updated_node_to_cbg, cbg_to_centroid)\n",
    "                # 计算平均聚类系数\n",
    "                avg_cluster_coeff = nx.average_clustering(graph.to_undirected())\n",
    "                average_cluster_coeffs.append(avg_cluster_coeff)\n",
    "            # 将结果存储到 DataFrame\n",
    "            for i in range(len(ks)):\n",
    "                results_df.loc[len(results_df)] = [year, month, city, ks[i], average_cluster_coeffs[i]]\n",
    "    results_df.to_csv('k_select_avg_distance_cluster_{}.csv'.format(city))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of the effective number of destinations (END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cities = ['New York, NY', 'Los Angeles, CA', 'Chicago, IL','Phoenix, AZ', 'Philadelphia, PA', 'San Diego, CA', 'Dallas, TX', 'San Jose, CA']\n",
    "city_list = list(range(1,9))\n",
    "\n",
    "df_effec_dest = pd.DataFrame(columns=[\"year\",\"month\"]+cities)\n",
    "\n",
    "for year in range(2018,2022):\n",
    "    for month in range(1,13):\n",
    "        mean_lst = [year, month]\n",
    "        for city in city_list:\n",
    "            flow_matrix = np.load(r'.\\data\\Mobility\\cbg_visit_{year_}-{month_}_{city_}.npy'.format(year_ = year, month_ = str(month).zfill(2), city_ = city))\n",
    "            #print(city, cities[city-1])\n",
    "            \n",
    "            effective_dests = []\n",
    "            for i in range(flow_matrix.shape[0]):\n",
    "\n",
    "                sum = np.sum(flow_matrix[i])\n",
    "                if sum == 0:\n",
    "                    continue\n",
    "                tmp_vec = flow_matrix[i]/sum\n",
    "                effec_dest = 1 / np.sum(tmp_vec*tmp_vec)\n",
    "                effective_dests.append(effec_dest)\n",
    "            mean = np.mean(effective_dests)\n",
    "            mean_lst.append(mean)\n",
    "            print(cities[city-1], mean)\n",
    "        df_effec_dest.loc[len(df_effec_dest)] = mean_lst\n",
    "df_effec_dest.to_csv(r'.\\results\\effective_destinations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zwy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
