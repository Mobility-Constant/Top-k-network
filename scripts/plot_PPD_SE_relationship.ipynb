{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from shapely import wkt\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "plt.rcParams['font.family'] = ['Arial']\n",
    "colors = [\"#ca0020\", \"#f4a582\", \"#f7f7f7\", \"#92c5de\", \"#0571b0\"]\n",
    "custom_cmap_RDBU = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load socioeconomic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file path to socioeconomic data\n",
    "population_data=pd.read_csv('.\\data\\population_no_gini.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set 17 socioeconomic factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_list_new=['Median Household Income', 'A Ratio Of Income To Poverty Level Between 0.5 And 0.99', 'A Ratio Of Income To Poverty Level Below 0.5', 'noInsurPct', 'Average Population Enrolled In College', 'Average Population That Graduated From High School', \"Average Population With A Bachelor's Degree\", \"Average Population With A Master's Degree\", 'Average Population With A Doctorate', 'Building Density', 'Driving Road Density', 'Cycling Road Density', 'Walking Road Density', 'POI Density',  'Construction', 'Residential', 'Entropy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the k* value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file path to k values\n",
    "allk=pd.read_csv('.\\data\\k_select_US_newNY_2percent.csv')#k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the geographic basic information, and process population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('.\\data\\Basic_Geographic_Statistics_CBG.csv')\n",
    "\n",
    "data['Centroid'] = data['Centroid'].apply(wkt.loads)\n",
    "data['Boundary'] = data['Boundary'].apply(wkt.loads)\n",
    "\n",
    "geo_data = gpd.GeoDataFrame(data, geometry='Boundary')\n",
    "geo_data = geo_data.drop('Centroid', axis=1)\n",
    "\n",
    "year=2018\n",
    "geo_data_by_year = geo_data[geo_data['Year'] == year]\n",
    "# geo_data_by_year.head()\n",
    "population_data_with_geo=population_data.merge(geo_data_by_year, on=['CBG Code','Year','City Name'])\n",
    "\n",
    "population_data=population_data_with_geo\n",
    "\n",
    "#Average education level weighted by population\n",
    "population_data['Average Population Enrolled In College']=population_data['Population Enrolled In College']/population_data['Population']\n",
    "population_data['Average Population That Graduated From High School']=population_data['Population That Graduated From High School']/population_data['Population']\n",
    "population_data[\"Average Population With A Bachelor's Degree\"]=population_data[\"Population With A Bachelor's Degree\"]/population_data['Population']\n",
    "population_data[\"Average Population With A Master's Degree\"]=population_data[\"Population With A Master's Degree\"]/population_data['Population']\n",
    "population_data['Average Population With A Doctorate']=population_data['Population With A Doctorate']/population_data['Population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some parameters\n",
    "map_city = {}\n",
    "map_city[1] = \"New York, NY\"\n",
    "map_city[2] = \"Los Angeles, CA\"\n",
    "map_city[3] = \"Chicago, IL\"\n",
    "map_city[4] = \"Houston, TX\"\n",
    "map_city[5] = \"Phoenix, AZ\"\n",
    "map_city[6] = \"Philadelphia, PA\"\n",
    "map_city[7] = \"San Antonio, TX\"\n",
    "map_city[8] = \"San Diego, CA\"\n",
    "map_city[9] = \"Dallas, TX\"\n",
    "map_city[10] = \"San Jose, CA\"\n",
    "\n",
    "percentile=2\n",
    "year=2018\n",
    "month=6\n",
    "city_list=[1,2,3,5,6,8,9,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find cbgs in the  water \n",
    "This is because there are some cgbs that are in the water in our original dataset (id_dict and flow matrix). This phenomenon is mostly in NewYork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbgs = gpd.read_file(r\".\\arcgis project\\cbgs\\cbgs_of_cities\\NewYork\\New_York_city.shp\")\n",
    "with open(r\".\\data\\Mobility\\id_dict_1.pkl\", \"rb\") as f:\n",
    "    id_dict = pickle.load(f)\n",
    "\n",
    "print(cbgs.info())\n",
    "print(len(id_dict.keys()))\n",
    "\n",
    "# Extract the lines that not in cbgs but in id_dict\n",
    "id_dict_keys = list(id_dict.keys())\n",
    "id_dict_values = list(id_dict.values())\n",
    "cbgs_keys = list(cbgs[\"CBG_Code\"].values)\n",
    "cbgs_in_water = []\n",
    "ny_left_cbgs = []\n",
    "count = 0\n",
    "for i in id_dict_keys:\n",
    "    if id_dict[id_dict_keys[i]] not in cbgs_keys:\n",
    "        cbgs_in_water.append(i)\n",
    "        count += 1\n",
    "    else:\n",
    "        ny_left_cbgs.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_ratio(matrix, k):\n",
    "    # calculate the Proportion of Principal Destinations of each CBG for the k most frequent destinations\n",
    "    ratios = []\n",
    "    for row in matrix:\n",
    "        top_k_flow = np.sum(\n",
    "            np.sort(row)[-k:]\n",
    "        )  # 对每行排序并取最后k个元素的和（最大的k个流量）\n",
    "        total_flow = np.sum(row)  # 计算总流量\n",
    "        ratio = (\n",
    "            top_k_flow / total_flow if total_flow != 0 else 0\n",
    "        )  # 计算比例，避免零除错误\n",
    "        ratios.append(ratio)\n",
    "    return np.array(ratios)\n",
    "\n",
    "\n",
    "def process_cbg_visit_matrix(year, month, city_id, percentile):\n",
    "    # preprocess the mobility network to remove unimportant nodes\n",
    "    # Load the CBG visit matrix\n",
    "    filename = f\".\\data\\Mobility\\cbg_visit_{year}-{month:02}_{city_id}.npy\"\n",
    "    cbg_visit_matrix = np.load(filename)\n",
    "    # print(cbg_visit_matrix)\n",
    "    np.fill_diagonal(cbg_visit_matrix, 0)\n",
    "\n",
    "    # Remove the cbgs in the water in New York\n",
    "    if city_id == 1:\n",
    "        cbg_visit_matrix = np.delete(cbg_visit_matrix, cbgs_in_water, axis=0)\n",
    "        cbg_visit_matrix = np.delete(cbg_visit_matrix, cbgs_in_water, axis=1)\n",
    "        node_id_mapping_1 = {\n",
    "            without_water_id: original_id\n",
    "            for without_water_id, original_id in enumerate(ny_left_cbgs)\n",
    "        }\n",
    "\n",
    "    # Create directed graph from flow matrix\n",
    "    G = nx.from_numpy_array(cbg_visit_matrix, create_using=nx.DiGraph)\n",
    "    # Convert graph to numpy adjacency matrix\n",
    "    adj_matrix = nx.adjacency_matrix(G)\n",
    "    # print(adj_matrix)\n",
    "    numpy_array = adj_matrix.toarray()\n",
    "    # Convert the original flows to binary\n",
    "    flows_binary = (numpy_array > 0).astype(int)\n",
    "    # Calculate the degrees based on the new definition 相当于转化成无向图然后算的度？？\n",
    "    degrees_all = np.sum(np.logical_or(flows_binary, flows_binary.T), axis=1)\n",
    "    # print(degrees_all)\n",
    "    # Determine the 2% degree threshold\n",
    "    percentile_threshold = np.percentile(degrees_all, percentile)\n",
    "    # find the nodes whose degree are in the lowest 2%\n",
    "    nodes_to_keep = np.where(degrees_all > percentile_threshold)[0]\n",
    "    new_cbg_visit_matrix = cbg_visit_matrix[nodes_to_keep, :][:, nodes_to_keep]\n",
    "\n",
    "    if city_id == 1:\n",
    "        node_id_mapping_2 = {\n",
    "            new_id: without_water_id\n",
    "            for new_id, without_water_id in enumerate(nodes_to_keep)\n",
    "        }\n",
    "        # 从node_id_mapping_2到node_id_mapping_1的映射\n",
    "        node_id_mapping = {}\n",
    "        for new_id in node_id_mapping_2.keys():\n",
    "            node_id_mapping[new_id] = node_id_mapping_1[node_id_mapping_2[new_id]]\n",
    "        return new_cbg_visit_matrix, node_id_mapping\n",
    "\n",
    "    # Record the mapping from new node IDs to original node IDs\n",
    "    node_id_mapping = {\n",
    "        new_id: original_id for new_id, original_id in enumerate(nodes_to_keep)\n",
    "    }\n",
    "\n",
    "    return new_cbg_visit_matrix, node_id_mapping\n",
    "\n",
    "\n",
    "def linearFitting(X, Y, city_id, attribute):\n",
    "    # implement linear regression between X and Y\n",
    "    X = np.asarray(X)\n",
    "    Y = np.asarray(Y)\n",
    "    # linear fitting and the confidence interval\n",
    "    # m, b, r_value, p_value, std_err = stats.linregress(X, Y)\n",
    "    if len(set(X)) == 1:  # 检查X的所有值是否相同\n",
    "        m = 0\n",
    "        b = sum(Y) / len(Y)  # Y的均值\n",
    "        # 这种情况p和r如何确定\n",
    "        r_value = 0\n",
    "        p_value = 1\n",
    "    else:\n",
    "        m, b, r_value, p_value, std_err = stats.linregress(X, Y)\n",
    "\n",
    "    return m, b, r_value**2, p_value\n",
    "\n",
    "\n",
    "def explore_relationship_city_attribute_pop_weighted(\n",
    "    year, month, city_id, attribute, rank=True\n",
    "):\n",
    "    # get regression results and correlation coefficient between attribute and PPD for a city\n",
    "    with open(f\".\\data\\Mobility\\id_dict_{city_id}.pkl\", \"rb\") as f:\n",
    "        id_dict = pickle.load(f)\n",
    "    city_name_k = map_city[city_id]\n",
    "    k = allk.loc[(allk[\"month\"] == month) & (allk[\"year\"] == year), str(city_name_k)]\n",
    "    k = int(k)\n",
    "    cbg_visit_matrix, node_id_mapping = process_cbg_visit_matrix(\n",
    "        year, month, city_id, percentile\n",
    "    )\n",
    "    ratios = top_k_ratio(cbg_visit_matrix, k)\n",
    "    df = pd.DataFrame({\"id\": np.arange(len(ratios)), \"ratios\": ratios, \"Year\": year})\n",
    "    df[\"CBG Code\"] = 0\n",
    "    for ix, row in df.iterrows():\n",
    "        df.loc[ix, \"CBG Code\"] = id_dict[node_id_mapping[row[\"id\"]]]  # 得到cbg code\n",
    "    df = pd.merge(df, population_data, on=[\"CBG Code\", \"Year\"])\n",
    "    df = df.dropna()  # 去除nan\n",
    "    Y = df[\"ratios\"]\n",
    "    X = df[attribute]\n",
    "    X = X.rank(pct=rank)\n",
    "    m, b, r2, p_value = linearFitting(X, Y, city_id, attribute)\n",
    "    correlation_matrix = np.corrcoef(X, Y)\n",
    "    correlation_xy = correlation_matrix[0, 1]\n",
    "    return m, b, r2, p_value, correlation_xy\n",
    "\n",
    "\n",
    "def explore_relationship_city_attribute_anyk(year, month, city_id, attribute, k):\n",
    "    # get regression results and correlation coefficient between a socioeconomic attribute and Proportion of Principal Destinations for top k network in a city, for a arbitrary k\n",
    "    with open(f\".\\data\\Mobility\\id_dict_{city_id}.pkl\", \"rb\") as f:\n",
    "        id_dict = pickle.load(f)\n",
    "\n",
    "    cbg_visit_matrix, node_id_mapping = process_cbg_visit_matrix(\n",
    "        year, month, city_id, percentile\n",
    "    )\n",
    "    ratios = top_k_ratio(cbg_visit_matrix, k)\n",
    "    df = pd.DataFrame({\"id\": np.arange(len(ratios)), \"ratios\": ratios, \"Year\": year})\n",
    "    df[\"CBG Code\"] = 0\n",
    "    for ix, row in df.iterrows():\n",
    "        df.loc[ix, \"CBG Code\"] = id_dict[node_id_mapping[row[\"id\"]]]  # 得到cbg code\n",
    "    df = pd.merge(df, population_data, on=[\"CBG Code\", \"Year\"])\n",
    "\n",
    "    df = df.dropna() \n",
    "    Y = df[\"ratios\"]\n",
    "    X = df[attribute]\n",
    "    X = X.rank(pct=True)\n",
    "    m, b, r2, p_value = linearFitting(X, Y, city_id, attribute)\n",
    "\n",
    "    correlation_matrix = np.corrcoef(X, Y)\n",
    "    correlation_xy = correlation_matrix[0, 1]\n",
    "    return m, b, r2, p_value, correlation_xy\n",
    "\n",
    "#Set the drawing parameters, controlling the position of geographical boundary of each city\n",
    "view_extent_list = [0.37, 0.4, 0.34, 0.52, 0.22, 0.6, 0.4, 0.35]\n",
    "\n",
    "def explore_percolation_transition_share_single_city(year, month, city_id, percentile, ax,idx):\n",
    "    with open(f'Mobility/id_dict_{city_id}.pkl', 'rb') as f:\n",
    "        id_dict = pickle.load(f)\n",
    "    city_name_k=map_city[city_id]\n",
    "    k = allk.loc[(allk['month'] == month) & (allk['year'] == year), str(city_name_k)]\n",
    "    k=int(k)\n",
    "    print(\"k: \",k)\n",
    "    new_cbg_visit_matrix, node_id_mapping = process_cbg_visit_matrix(year, month, city_id, percentile)\n",
    "    #top k ratio\n",
    "    ratios = top_k_ratio(new_cbg_visit_matrix, k)\n",
    "    df = pd.DataFrame({'id': np.arange(len(ratios)),'ratios': ratios,'Year':year})\n",
    "    df['CBG Code']=0\n",
    "    for ix, row in df.iterrows():\n",
    "        df.loc[ix,'CBG Code']=id_dict[node_id_mapping[row['id']]]#得到cbg code\n",
    "    df = df.dropna()\n",
    "\n",
    "    geo_data_by_year_share = geo_data_by_year.merge(df, on=['CBG Code'])\n",
    "\n",
    "    output_filename = f\"spatial_distribution_PPD_{city_name_k}_{year}_{month}_percentile{percentile}.shp\"\n",
    "    geo_data_by_year_share.to_file(output_filename)\n",
    "    plot = geo_data_by_year_share.plot(column='ratios', cmap=custom_cmap_RDBU, ax=ax, vmin=0.6, vmax=1.0, legend=False)\n",
    "    bounds = geo_data_by_year_share.total_bounds  # 返回 [minx, miny, maxx, maxy]\n",
    "    map_center_x = (bounds[0] + bounds[2]) / 2\n",
    "    map_center_y = (bounds[1] + bounds[3]) / 2\n",
    "    view_extent = view_extent_list[idx] \n",
    "    ax.set_xlim(map_center_x - view_extent, map_center_x + view_extent)\n",
    "    ax.set_ylim(map_center_y - view_extent, map_center_y + view_extent)\n",
    "    ax.axis('off')\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Heatmap\n",
    "\n",
    "Correlation Analysis of Socioeconomic Factors with the Proportion of Principal Destinations (PPD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data for ploting\n",
    "df_results_all = {}\n",
    "for city_id in city_list:\n",
    "    # print(city_id)\n",
    "    results = []\n",
    "    for i in range(len(attributes_list_new)):\n",
    "        attribute = attributes_list_new[i]\n",
    "        # print(year,month,city_id,attribute)\n",
    "        m, b, r2, p_value, correlation_xy=explore_relationship_city_attribute_pop_weighted(year, month, city_id, attribute, rank=True)\n",
    "        results.append([attribute, m, b, r2, p_value, correlation_xy])\n",
    "\n",
    "    df_results = pd.DataFrame(results, columns=[\"Attribute\", \"Slope\", \"Intercept\", \"R2\",\"p-value\", \"Correlation Coefficient\"])\n",
    "    df_results_all[city_id] = df_results\n",
    "\n",
    "data_dict = {}\n",
    "selected_attributes_dict={}\n",
    "\n",
    "for city_id in city_list:\n",
    "    df = df_results_all[city_id]\n",
    "    selected_attributes_city = df[df['p-value'] <= 0.05]['Attribute'].tolist()\n",
    "    selected_attributes_dict[city_id]=selected_attributes_city\n",
    "    data_dict[city_id] = df.set_index('Attribute')['Correlation Coefficient'].to_dict()\n",
    "# print(data_dict)\n",
    "matrix = []\n",
    "for city_id in city_list:\n",
    "    row = [data_dict[city_id].get(attr, 0) if attr in selected_attributes_dict[city_id] else 0 for attr in attributes_list_new]\n",
    "    matrix.append(row)\n",
    "\n",
    "matrix = np.array(matrix)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10),dpi=300)\n",
    "\n",
    "c = ax.imshow(matrix, cmap='RdBu_r', vmin=-0.7, vmax=0.7)#红蓝改了顺序\n",
    "\n",
    "for idx, attr in enumerate(attributes_list_new):\n",
    "    ax.text(idx, -1, str(idx+1), ha='center', va='center', color='black', fontsize=10, fontweight='bold')\n",
    "\n",
    "print([city_name:=map_city[i] for i in city_list])\n",
    "ax.set_yticks(np.arange(8))\n",
    "ax.set_yticklabels([city_name:=map_city[i] for i in city_list], fontweight='bold')\n",
    "ax.set_xticks(np.arange(len(attributes_list_new)))\n",
    "ax.set_xticklabels([])  \n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "# id_to_attribute = {i+1: attr for i, attr in enumerate(attributes_list_new)}\n",
    "id_to_attribute = {i+1: (attr.replace(\"A \", \"The \") if attr.startswith(\"A Ratio\") else attr) for i, attr in enumerate(attributes_list_new)}\n",
    "\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', markersize=10, label=f\"{i}:  {attr}\") for i, attr in id_to_attribute.items()]\n",
    "\n",
    "legend = plt.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.4, 1), labelspacing=1.1, prop={'weight': 'bold'})\n",
    "\n",
    "\n",
    "axins = inset_axes(ax,\n",
    "                width=\"5%\",  \n",
    "                height=\"30%\",  \n",
    "                loc='lower left',\n",
    "                bbox_to_anchor=(-0.2, 0.1, 1, 1),\n",
    "                bbox_transform=ax.transAxes,\n",
    "                borderpad=0,\n",
    "                )\n",
    "fig.colorbar(c, cax=axins,orientation='vertical', pad=0.1, shrink=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"Heatmap_{year}_{month:02}.png\",bbox_inches='tight',transparent=True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce data for the significant shift in the correlation coefficients around the critical connectivity threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = list(range(50, 551, 10))\n",
    "for city_id in city_list:\n",
    "    for k in range(50, 551, 10):\n",
    "        print(\"city_id: \",city_id,\"month: \",month,\"k: \",k)\n",
    "        results = []\n",
    "        for i in range(len(attributes_list_new)):\n",
    "            attribute = attributes_list_new[i]\n",
    "            m, b, r2, p_value, correlation_xy=explore_relationship_city_attribute_anyk(year, month, city_id, attribute, k)\n",
    "            results.append([attribute, m, b, r2, p_value, correlation_xy])\n",
    "        df_results = pd.DataFrame(results, columns=[\"Attribute\", \"Slope\", \"Intercept\", \"R2\",\"p-value\", \"Correlation Coefficient\"])\n",
    "\n",
    "        data_dir = \".\\data\\arbitrary_k\"\n",
    "        if not os.path.exists(data_dir):\n",
    "            os.makedirs(data_dir)\n",
    "        df_results.to_csv(f\"{data_dir}\\Results_{city_id}_{year}_{month}_kis{k}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the significant shift graph\n",
    "\n",
    "Plot the graph on the significant shift in the correlation coefficients around the critical connectivity threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attMarkersNew = {\"Median Household Income\": \"o\", \\\n",
    "    \"A Ratio Of Income To Poverty Level Between 0.5 And 0.99\": \"s\",\\\n",
    "    \"A Ratio Of Income To Poverty Level Below 0.5\": \"^\", \\\n",
    "    \"noInsurPct\": \"H\", \"Average Population Enrolled In College\": \"D\", \"Average Population That Graduated From High School\": \"P\", \\\n",
    "    \"Average Population With A Bachelor's Degree\": \"<\", \"Average Population With A Master's Degree\": \"X\", \"Average Population With A Doctorate\": \"v\",\\\n",
    "    \"Building Density\": \"d\", \"Driving Road Density\": \"*\",\n",
    "    \"Cycling Road Density\": \"o\", \"Walking Road Density\": \"s\",\n",
    "    \"POI Density\": \"^\", \"Construction\": \"H\", \"Residential\": \"D\",\n",
    "    \"Entropy\": \"P\"}\n",
    "\n",
    "attMarkerSizeNew={\"Median Household Income\": 3, \\\n",
    "    \"A Ratio Of Income To Poverty Level Between 0.5 And 0.99\": 3,\\\n",
    "    \"A Ratio Of Income To Poverty Level Below 0.5\": 3, \\\n",
    "    \"noInsurPct\": 4, \"Average Population Enrolled In College\": 3, \"Average Population That Graduated From High School\": 4, \\\n",
    "    \"Average Population With A Bachelor's Degree\": 3, \"Average Population With A Master's Degree\": 3, \"Average Population With A Doctorate\": 3,\\\n",
    "    \"Building Density\": 3, \"Driving Road Density\": 4.5,\n",
    "    \"Cycling Road Density\": 3, \"Walking Road Density\": 3,\n",
    "    \"POI Density\": 3, \"Construction\": 4, \"Residential\": 3,\n",
    "    \"Entropy\": 4}\n",
    "\n",
    "\n",
    "for city_id in city_list:\n",
    "    data_dir = \".\\data\\arbitrary_k\"\n",
    "    df2=df_results_all[city_id]\n",
    "    selected_attributes = df2[df2['p-value'] <= 0.05]['Attribute'].tolist()\n",
    "    city_name_k=map_city[city_id]\n",
    "    realk = allk.loc[(allk['month'] == month) & (allk['year'] == year), str(city_name_k)]\n",
    "    realk=int(realk)\n",
    "    results_dict = {}\n",
    "    k_values = list(range(50, 411, 10))\n",
    "    for k in range(50, 411, 10):\n",
    "        filename = f\"{data_dir}\\Results_{city_id}_{year}_{month}_kis{k}.csv\"\n",
    "        df = pd.read_csv(filename)\n",
    "        for attribute in attributes_list_new:\n",
    "            if attribute not in results_dict:\n",
    "                results_dict[attribute] = []\n",
    "            results_dict[attribute].append(df[df['Attribute'] == attribute]['p-value'].values[0])\n",
    "    \n",
    "    colors = plt.cm.tab20.colors \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4.5), dpi=300)\n",
    "    adjusted_colors = [color for i, color in enumerate(colors) if (i != 15 and i!=16)]\n",
    "\n",
    "    for idx, (attribute, p_values) in enumerate(results_dict.items()):\n",
    "        # print(attribute)\n",
    "        if attribute not in selected_attributes:\n",
    "            ax.plot(k_values, p_values,alpha=0.4, label=attribute, marker='x', color=colors[15],markersize=3, linewidth=1)  \n",
    "        else:\n",
    "            attr=attribute.replace(\"A \", \"The \") if attribute.startswith(\"A Ratio\") else attribute\n",
    "            ax.plot(k_values, p_values,alpha=0.7, label=f\"{attr}\", marker=attMarkersNew[attribute], color=adjusted_colors[idx % len(adjusted_colors)],markersize=attMarkerSizeNew[attribute],linewidth=1)\n",
    "    ax.axhline(y=0.05, color='#61C9A8', linestyle='--')\n",
    "    ax.axvline(x=realk, color='black', linestyle='--')\n",
    "    ax.text(realk-10, 0.98, '     $k*$', fontsize=10)\n",
    "    ax.set_xlim(50, 411)\n",
    "    ax.set_xlabel('k value')\n",
    "    ax.set_ylabel('p-value')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), labelspacing=1.0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Set the output directory and file name\n",
    "    output_file = f\"Significance_{map_city[city_id]}_{year}_{month}.png\"\n",
    "    plt.savefig(output_file, bbox_inches='tight', transparent=True)  # Save the figure to the specified directory\n",
    "    plt.close(fig)  # Close the figure after saving to free up memory\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot spatial distribution of Proportion of Principal Destinations (PPD) for New York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2018\n",
    "month = 6\n",
    "city_id = 1\n",
    "percentile = 2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 5), dpi=300) \n",
    "explore_percolation_transition_share_single_city(year, month, city_id, percentile, ax, 0)\n",
    "cmap = custom_cmap_RDBU  \n",
    "norm = colors.Normalize(vmin=0.6, vmax=1.0)\n",
    "\n",
    "sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=ax, orientation='vertical', fraction=0.02, pad=0.09) \n",
    "cbar.set_label('PPD', fontsize=12)\n",
    "\n",
    "plt.savefig(f\"PPD_spatial_distribution_{map_city[city_id]}_{year}_{month}.png\", bbox_inches='tight', transparent=True) \n",
    "plt.tight_layout()  \n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
